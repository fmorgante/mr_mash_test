<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Fabio Morgante" />

<meta name="date" content="2020-06-02" />

<title>Dense case</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.0.13/css/fa-svg-with-js.css" rel="stylesheet" />
<script src="site_libs/font-awesome-5.0.13/js/fontawesome-all.min.js"></script>
<script src="site_libs/font-awesome-5.0.13/js/fa-v4-shims.min.js"></script>

<link rel="icon" href="https://github.com/workflowr/workflowr-assets/raw/master/img/reproducible.png">
<!-- Add a small amount of space between sections. -->
<style type="text/css">
div.section {
  padding-top: 12px;
}
</style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">mr_mash_test</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/fmorgante/mr_mash_test">
    <span class="fa fa-github"></span>
     
    Source code
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Dense case</h1>
<h4 class="author"><em>Fabio Morgante</em></h4>
<h4 class="date"><em>June 02, 2020</em></h4>

</div>


<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-report" data-toggle="collapse" data-target="#workflowr-report">
<span class="glyphicon glyphicon-list" aria-hidden="true"></span> workflowr <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span>
</button>
</p>
<div id="workflowr-report" class="collapse">
<ul class="nav nav-tabs">
<li class="active">
<a data-toggle="tab" href="#summary">Summary</a>
</li>
<li>
<a data-toggle="tab" href="#checks"> Checks <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> </a>
</li>
<li>
<a data-toggle="tab" href="#versions">Past versions</a>
</li>
</ul>
<div class="tab-content">
<div id="summary" class="tab-pane fade in active">
<p>
<strong>Last updated:</strong> 2020-06-02
</p>
<p>
<strong>Checks:</strong> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> 7 <span class="glyphicon glyphicon-exclamation-sign text-danger" aria-hidden="true"></span> 0
</p>
<p>
<strong>Knit directory:</strong> <code>mr_mash_test/</code> <span class="glyphicon glyphicon-question-sign" aria-hidden="true" title="This is the local directory in which the code in this file was executed."> </span>
</p>
<p>
This reproducible <a href="http://rmarkdown.rstudio.com">R Markdown</a> analysis was created with <a
  href="https://github.com/jdblischak/workflowr">workflowr</a> (version 1.6.1). The <em>Checks</em> tab describes the reproducibility checks that were applied when the results were created. The <em>Past versions</em> tab lists the development history.
</p>
<hr>
</div>
<div id="checks" class="tab-pane fade">
<div id="workflowr-checks" class="panel-group">
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRMarkdownfilestronguptodate"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>R Markdown file:</strong> up-to-date </a>
</p>
</div>
<div id="strongRMarkdownfilestronguptodate" class="panel-collapse collapse">
<div class="panel-body">
<p>Great! Since the R Markdown file has been committed to the Git repository, you know the exact version of the code that produced these results.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongEnvironmentstrongempty"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Environment:</strong> empty </a>
</p>
</div>
<div id="strongEnvironmentstrongempty" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! The global environment was empty. Objects defined in the global environment can affect the analysis in your R Markdown file in unknown ways. For reproduciblity it’s best to always run the code in an empty environment.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSeedstrongcodesetseed20200328code"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Seed:</strong> <code>set.seed(20200328)</code> </a>
</p>
</div>
<div id="strongSeedstrongcodesetseed20200328code" class="panel-collapse collapse">
<div class="panel-body">
<p>The command <code>set.seed(20200328)</code> was run prior to running the code in the R Markdown file. Setting a seed ensures that any results that rely on randomness, e.g. subsampling or permutations, are reproducible.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongSessioninformationstrongrecorded"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Session information:</strong> recorded </a>
</p>
</div>
<div id="strongSessioninformationstrongrecorded" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Recording the operating system, R version, and package versions is critical for reproducibility.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongCachestrongnone"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Cache:</strong> none </a>
</p>
</div>
<div id="strongCachestrongnone" class="panel-collapse collapse">
<div class="panel-body">
<p>Nice! There were no cached chunks for this analysis, so you can be confident that you successfully produced the results during this run.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongFilepathsstrongrelative"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>File paths:</strong> relative </a>
</p>
</div>
<div id="strongFilepathsstrongrelative" class="panel-collapse collapse">
<div class="panel-body">
<p>Great job! Using relative paths to the files within your workflowr project makes it easier to run your code on other machines.</p>
</div>
</div>
</div>
<div class="panel panel-default">
<div class="panel-heading">
<p class="panel-title">
<a data-toggle="collapse" data-parent="#workflowr-checks" href="#strongRepositoryversionstrongahrefhttpsgithubcomfmorgantemrmashtesttree3c70b54566a4202eb584ca69e4e96b7cd7870199targetblank3c70b54a"> <span class="glyphicon glyphicon-ok text-success" aria-hidden="true"></span> <strong>Repository version:</strong> <a href="https://github.com/fmorgante/mr_mash_test/tree/3c70b54566a4202eb584ca69e4e96b7cd7870199" target="_blank">3c70b54</a> </a>
</p>
</div>
<div id="strongRepositoryversionstrongahrefhttpsgithubcomfmorgantemrmashtesttree3c70b54566a4202eb584ca69e4e96b7cd7870199targetblank3c70b54a" class="panel-collapse collapse">
<div class="panel-body">
<p>
Great! You are using Git for version control. Tracking code development and connecting the code version to the results is critical for reproducibility.
</p>
<p>
The results in this page were generated with repository version <a href="https://github.com/fmorgante/mr_mash_test/tree/3c70b54566a4202eb584ca69e4e96b7cd7870199" target="_blank">3c70b54</a>. See the <em>Past versions</em> tab to see a history of the changes made to the R Markdown and HTML files.
</p>
<p>
Note that you need to be careful to ensure that all relevant files for the analysis have been committed to Git prior to generating the results (you can use <code>wflow_publish</code> or <code>wflow_git_commit</code>). workflowr only checks the R Markdown file, but you know if there are other scripts or data files that it depends on. Below is the status of the Git repository when the results were generated:
</p>
<pre><code>
Ignored files:
    Ignored:    .sos/
    Ignored:    code/fit_mr_mash.66662433.err
    Ignored:    code/fit_mr_mash.66662433.out
    Ignored:    dsc/.sos/
    Ignored:    dsc/outfiles/
    Ignored:    output/dsc.html
    Ignored:    output/dsc/
    Ignored:    output/dsc_05_18_20.html
    Ignored:    output/dsc_05_18_20/
    Ignored:    output/dsc_05_29_20.html
    Ignored:    output/dsc_05_29_20/
    Ignored:    output/dsc_OLD.html
    Ignored:    output/dsc_OLD/
    Ignored:    output/dsc_test.html
    Ignored:    output/dsc_test/

Untracked files:
    Untracked:  code/plot_test.R

Unstaged changes:
    Modified:   dsc/midway2.yml

</code></pre>
<p>
Note that any generated files, e.g. HTML, png, CSS, etc., are not included in this status report because it is ok for generated content to have uncommitted changes.
</p>
</div>
</div>
</div>
</div>
<hr>
</div>
<div id="versions" class="tab-pane fade">

<p>
These are the previous versions of the repository in which changes were made to the R Markdown (<code>analysis/dense_scenario_issue.Rmd</code>) and HTML (<code>docs/dense_scenario_issue.html</code>) files. If you’ve configured a remote Git repository (see <code>?wflow_git_remote</code>), click on the hyperlinks in the table below to view the files as they were in that past version.
</p>
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
File
</th>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
<th>
Message
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/fmorgante/mr_mash_test/blob/3c70b54566a4202eb584ca69e4e96b7cd7870199/analysis/dense_scenario_issue.Rmd" target="_blank">3c70b54</a>
</td>
<td>
fmorgante
</td>
<td>
2020-06-02
</td>
<td>
Update dense scenario
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/fmorgante/mr_mash_test/2ace633ad122eb0a8919a385a60a256d4d4312fc/docs/dense_scenario_issue.html" target="_blank">2ace633</a>
</td>
<td>
fmorgante
</td>
<td>
2020-06-02
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/fmorgante/mr_mash_test/blob/654b67ad7311f918875f13dd82753f66303d3958/analysis/dense_scenario_issue.Rmd" target="_blank">654b67a</a>
</td>
<td>
fmorgante
</td>
<td>
2020-06-02
</td>
<td>
Add additional simulation to the dense scenario
</td>
</tr>
<tr>
<td>
html
</td>
<td>
<a href="https://rawcdn.githack.com/fmorgante/mr_mash_test/42ec51cccb6bdad3adfe121582e30dcf3a409008/docs/dense_scenario_issue.html" target="_blank">42ec51c</a>
</td>
<td>
fmorgante
</td>
<td>
2020-06-01
</td>
<td>
Build site.
</td>
</tr>
<tr>
<td>
Rmd
</td>
<td>
<a href="https://github.com/fmorgante/mr_mash_test/blob/ceda27823b4d8ad4eb93fce2d909fe16c942d909/analysis/dense_scenario_issue.Rmd" target="_blank">ceda278</a>
</td>
<td>
fmorgante
</td>
<td>
2020-06-01
</td>
<td>
Add case study with dense scenario
</td>
</tr>
</tbody>
</table>
</div>
<hr>
</div>
</div>
</div>
<pre class="r"><code>library(mr.mash.alpha)
library(glmnet)</code></pre>
<pre><code>Loading required package: Matrix</code></pre>
<pre><code>Loading required package: foreach</code></pre>
<pre><code>Loaded glmnet 2.0-16</code></pre>
<pre class="r"><code>###Set options
options(stringsAsFactors = FALSE)

###Function to compute accuracy
compute_accuracy &lt;- function(Y, Yhat) {
  bias &lt;- rep(as.numeric(NA), ncol(Y))
  r2 &lt;- rep(as.numeric(NA), ncol(Y))
  mse &lt;- rep(as.numeric(NA), ncol(Y))
  
  for(i in 1:ncol(Y)){
    fit  &lt;- lm(Y[, i] ~ Yhat[, i])
    bias[i] &lt;- coef(fit)[2] 
    r2[i] &lt;- summary(fit)$r.squared
    mse[i] &lt;- mean((Y[, i] - Yhat[, i])^2)
  }
  
  return(list(bias=bias, r2=r2, mse=mse))
}</code></pre>
<div id="dense-scenario" class="section level3">
<h3>Dense scenario</h3>
<p>Here we investigate why <em>mr.mash</em> does not perform as well in denser scenarios. Let’s simulate data with n=600, p=1,000, p_causal=500, r=5, r_causal=5, PVE=0.5, shared effects, independent predictors, and independent residuals. The models will be fitted to the training data (80% of the full data).</p>
<pre class="r"><code>###Set seed
set.seed(123)

###Set parameters
n &lt;- 600
p &lt;- 1000
p_causal &lt;- 500
r &lt;- 5
r_causal &lt;- r
pve &lt;- 0.5
B_cor &lt;- 1
X_cor &lt;- 0
V_cor &lt;- 0

###Simulate V, B, X and Y
out &lt;- mr.mash.alpha:::simulate_mr_mash_data(n, p, p_causal, r, r_causal=r, intercepts = rep(1, r),
                                             pve=pve, B_cor=B_cor, B_scale=0.9, X_cor=X_cor, X_scale=0.8, V_cor=V_cor)

colnames(out$Y) &lt;- paste0(&quot;Y&quot;, seq(1, r))
rownames(out$Y) &lt;- paste0(&quot;N&quot;, seq(1, n))
colnames(out$X) &lt;- paste0(&quot;X&quot;, seq(1, p))
rownames(out$X) &lt;- paste0(&quot;N&quot;, seq(1, n))

###Split the data in training and test sets
test_set &lt;- sort(sample(x=c(1:n), size=round(n*0.2), replace=FALSE))
Ytrain &lt;- out$Y[-test_set, ]
Xtrain &lt;- out$X[-test_set, ]
Ytest &lt;- out$Y[test_set, ]
Xtest &lt;- out$X[test_set, ]</code></pre>
<p>We build the mixture prior as usual including zero matrix, identity matrix, rank-1 matrices, low, medium and high heterogeneity matrices, shared matrix, each scaled by a grid from 0.1 to 2.1 in steps of 0.2.</p>
<pre class="r"><code>grid &lt;- seq(0.1, 2.1, 0.2)
S0 &lt;- mr.mash.alpha:::compute_cov_canonical(ncol(Ytrain), singletons=TRUE, hetgrid=c(0, 0.25, 0.5, 0.75, 1), grid, zeromat=TRUE)</code></pre>
<p>We run <em>glmnet</em> with <span class="math">\(\alpha=1\)</span> to obtain an inital estimate for the regression coefficients to provide to <em>mr.mash</em>, and for comparison.</p>
<pre class="r"><code>###Fit group-lasso
cvfit_glmnet &lt;- cv.glmnet(x=Xtrain, y=Ytrain, family=&quot;mgaussian&quot;, alpha=1)
coeff_glmnet &lt;- coef(cvfit_glmnet, s=&quot;lambda.min&quot;)
Bhat_glmnet &lt;- matrix(as.numeric(NA), nrow=p, ncol=r)
ahat_glmnet &lt;- vector(&quot;numeric&quot;, r)
for(i in 1:length(coeff_glmnet)){
  Bhat_glmnet[, i] &lt;- as.vector(coeff_glmnet[[i]])[-1]
  ahat_glmnet[i] &lt;- as.vector(coeff_glmnet[[i]])[1]
}
Yhat_test_glmnet &lt;- drop(predict(cvfit_glmnet, newx=Xtest, s=&quot;lambda.min&quot;))</code></pre>
<p>We run <em>mr.mash</em> with EM updates of the mixture weights, updating V, and initializing the regression coefficients with the estimates from <em>glmnet</em>.</p>
<pre class="r"><code>###Fit mr.mash
fit_mrmash &lt;- mr.mash(Xtrain, Ytrain, S0, tol=1e-2, convergence_criterion=&quot;ELBO&quot;, update_w0=TRUE, 
                     update_w0_method=&quot;EM&quot;, compute_ELBO=TRUE, standardize=TRUE, verbose=FALSE, update_V=TRUE,
                     mu1_init = Bhat_glmnet, w0_threshold=1e-8)</code></pre>
<pre><code>Processing the inputs... Done!
Fitting the optimization algorithm... Done!
Processing the outputs... Done!
mr.mash successfully executed in 2.560016 minutes!</code></pre>
<pre class="r"><code>Yhat_test_mrmash &lt;- predict(fit_mrmash, Xtest)</code></pre>
<pre class="r"><code>layout(matrix(c(1, 1, 2, 2,
                1, 1, 2, 2,
                0, 3, 3, 0,
                0, 3, 3, 0), 4, 4, byrow = TRUE))

###Plot estimated vs true coeffcients
##mr.mash
plot(out$B, fit_mrmash$mu1, main=&quot;mr.mash&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
##glmnet
plot(out$B, Bhat_glmnet, main=&quot;glmnet&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)

###Plot mr.mash vs glmnet estimated coeffcients
colorz &lt;- matrix(&quot;black&quot;, nrow=p, ncol=r)
zeros &lt;- apply(out$B, 2, function(x) x==0)
for(i in 1:ncol(colorz)){
  colorz[zeros[, i], i] &lt;- &quot;red&quot;
}

plot(Bhat_glmnet, fit_mrmash$mu1, main=&quot;mr.mash vs glmnet&quot;, 
     xlab=&quot;glmnet estimated coefficients&quot;, ylab=&quot;mr.mash estimated coefficients&quot;,
     col=colorz, cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
legend(&quot;topleft&quot;, 
       legend = c(&quot;Non-zero&quot;, &quot;Zero&quot;), 
       col = c(&quot;black&quot;, &quot;red&quot;), 
       pch = c(1, 1), 
       horiz = FALSE,
       cex=2) </code></pre>
<p><img src="figure/dense_scenario_issue.Rmd/plot%20results-1.png" width="1440" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-no-spaces-plot-results-1">
Past versions of “plot results-1.png”
</button>
</p>
<div id="fig-no-spaces-plot-results-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/fmorgante/mr_mash_test/blob/42ec51cccb6bdad3adfe121582e30dcf3a409008/docs/figure/dense_scenario_issue.Rmd/plot results-1.png" target="_blank">42ec51c</a>
</td>
<td>
fmorgante
</td>
<td>
2020-06-01
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>print(&quot;Prediction performance with glmnet&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with glmnet&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest, Yhat_test_glmnet)</code></pre>
<pre><code>$bias
[1] 0.7973441 0.7171240 1.1201318 0.9992656 0.9375002

$r2
[1] 0.12560024 0.10044653 0.14357016 0.13204939 0.09516285

$mse
[1] 502.2840 531.2713 573.5326 575.3974 722.7162</code></pre>
<pre class="r"><code>print(&quot;Prediction performance with mr.mash&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with mr.mash&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest, Yhat_test_mrmash)</code></pre>
<pre><code>$bias
[1] 2.268094 2.196013 3.435006 2.831555 3.026599

$r2
[1] 0.1310161 0.1128144 0.2345789 0.1833654 0.1535817

$mse
[1] 518.2328 530.0913 591.4375 587.3239 730.5908</code></pre>
<pre class="r"><code>print(&quot;True V (full data)&quot;)</code></pre>
<pre><code>[1] &quot;True V (full data)&quot;</code></pre>
<pre class="r"><code>out$V</code></pre>
<pre><code>         [,1]     [,2]     [,3]     [,4]     [,5]
[1,] 331.4137   0.0000   0.0000   0.0000   0.0000
[2,]   0.0000 331.4137   0.0000   0.0000   0.0000
[3,]   0.0000   0.0000 331.4137   0.0000   0.0000
[4,]   0.0000   0.0000   0.0000 331.4137   0.0000
[5,]   0.0000   0.0000   0.0000   0.0000 331.4137</code></pre>
<pre class="r"><code>print(&quot;Estimated V (training data)&quot;)</code></pre>
<pre><code>[1] &quot;Estimated V (training data)&quot;</code></pre>
<pre class="r"><code>fit_mrmash$V</code></pre>
<pre><code>         Y1       Y2       Y3       Y4       Y5
Y1 600.7891 271.7844 213.0465 197.5704 231.8269
Y2 271.7844 581.4331 244.8055 243.7165 255.7953
Y3 213.0465 244.8055 542.5477 252.0331 239.4896
Y4 197.5704 243.7165 252.0331 532.5570 214.1998
Y5 231.8269 255.7953 239.4896 214.1998 516.2954</code></pre>
<p>It looks <em>mr.mash</em> shrinks the coeffcients too much and the residual covariance is absorbing part of the effects. Let’s now try to fix the residual covariance to the truth (although it is going to be a little different since we are fitting the model to the training set only).</p>
<pre class="r"><code>###Fit mr.mash
fit_mrmash_fixV &lt;- mr.mash(Xtrain, Ytrain, S0, tol=1e-2, convergence_criterion=&quot;ELBO&quot;, update_w0=TRUE, 
                     update_w0_method=&quot;EM&quot;, compute_ELBO=TRUE, standardize=TRUE, verbose=FALSE, update_V=FALSE,
                     mu1_init = Bhat_glmnet, w0_threshold=1e-8, V=out$V)</code></pre>
<pre><code>Processing the inputs... Done!
Fitting the optimization algorithm... Done!
Processing the outputs... Done!
mr.mash successfully executed in 3.236134 minutes!</code></pre>
<pre class="r"><code>Yhat_test_mrmash_fixV &lt;- predict(fit_mrmash_fixV, Xtest)</code></pre>
<pre class="r"><code>layout(matrix(c(1, 1, 2, 2,
                1, 1, 2, 2,
                0, 3, 3, 0,
                0, 3, 3, 0), 4, 4, byrow = TRUE))

###Plot estimated vs true coeffcients
##mr.mash
plot(out$B, fit_mrmash_fixV$mu1, main=&quot;mr.mash&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
##glmnet
plot(out$B, Bhat_glmnet, main=&quot;glmnet&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)

###Plot mr.mash vs glmnet estimated coeffcients
colorz &lt;- matrix(&quot;black&quot;, nrow=p, ncol=r)
zeros &lt;- apply(out$B, 2, function(x) x==0)
for(i in 1:ncol(colorz)){
  colorz[zeros[, i], i] &lt;- &quot;red&quot;
}

plot(Bhat_glmnet, fit_mrmash_fixV$mu1, main=&quot;mr.mash vs glmnet&quot;, 
     xlab=&quot;glmnet estimated coefficients&quot;, ylab=&quot;mr.mash estimated coefficients&quot;,
     col=colorz, cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
legend(&quot;topleft&quot;, 
       legend = c(&quot;Non-zero&quot;, &quot;Zero&quot;), 
       col = c(&quot;black&quot;, &quot;red&quot;), 
       pch = c(1, 1), 
       horiz = FALSE,
       cex=2) </code></pre>
<p><img src="figure/dense_scenario_issue.Rmd/plot%20results%20fixed%20V-1.png" width="1440" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-no-spaces-plot-results-fixed-V-1">
Past versions of “plot results fixed V-1.png”
</button>
</p>
<div id="fig-no-spaces-plot-results-fixed-V-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/fmorgante/mr_mash_test/blob/42ec51cccb6bdad3adfe121582e30dcf3a409008/docs/figure/dense_scenario_issue.Rmd/plot results fixed V-1.png" target="_blank">42ec51c</a>
</td>
<td>
fmorgante
</td>
<td>
2020-06-01
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>print(&quot;Prediction performance with glmnet&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with glmnet&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest, Yhat_test_glmnet)</code></pre>
<pre><code>$bias
[1] 0.7973441 0.7171240 1.1201318 0.9992656 0.9375002

$r2
[1] 0.12560024 0.10044653 0.14357016 0.13204939 0.09516285

$mse
[1] 502.2840 531.2713 573.5326 575.3974 722.7162</code></pre>
<pre class="r"><code>print(&quot;Prediction performance with mr.mash&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with mr.mash&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest, Yhat_test_mrmash_fixV)</code></pre>
<pre><code>$bias
[1] 0.3739759 0.4820432 0.5484524 0.5637339 0.4956777

$r2
[1] 0.07025924 0.11696855 0.12904463 0.13990240 0.08790078

$mse
[1] 642.2267 599.7516 637.9042 632.1933 801.5090</code></pre>
<p>Prediction performance looks worse in terms of MSE and <span class="math">\(r^2\)</span> while bias, although still off, is closer to 1. However, looking at the plots of the coefficients, we can see that many coefficients are no longer overshrunk. Let’s see what happens if we initialize V to the truth but then we update it.</p>
<pre class="r"><code>###Fit mr.mash
fit_mrmash_initV &lt;- mr.mash(Xtrain, Ytrain, S0, tol=1e-2, convergence_criterion=&quot;ELBO&quot;, update_w0=TRUE, 
                     update_w0_method=&quot;EM&quot;, compute_ELBO=TRUE, standardize=TRUE, verbose=FALSE, update_V=TRUE,
                     mu1_init = Bhat_glmnet, w0_threshold=1e-8, V=out$V)</code></pre>
<pre><code>Processing the inputs... Done!
Fitting the optimization algorithm... Done!
Processing the outputs... Done!
mr.mash successfully executed in 2.579748 minutes!</code></pre>
<pre class="r"><code>Yhat_test_mrmash_initV &lt;- predict(fit_mrmash_initV, Xtest)</code></pre>
<pre class="r"><code>layout(matrix(c(1, 1, 2, 2,
                1, 1, 2, 2,
                0, 3, 3, 0,
                0, 3, 3, 0), 4, 4, byrow = TRUE))

###Plot estimated vs true coeffcients
##mr.mash
plot(out$B, fit_mrmash_initV$mu1, main=&quot;mr.mash&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
##glmnet
plot(out$B, Bhat_glmnet, main=&quot;glmnet&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)

###Plot mr.mash vs glmnet estimated coeffcients
colorz &lt;- matrix(&quot;black&quot;, nrow=p, ncol=r)
zeros &lt;- apply(out$B, 2, function(x) x==0)
for(i in 1:ncol(colorz)){
  colorz[zeros[, i], i] &lt;- &quot;red&quot;
}

plot(Bhat_glmnet, fit_mrmash_initV$mu1, main=&quot;mr.mash vs glmnet&quot;, 
     xlab=&quot;glmnet estimated coefficients&quot;, ylab=&quot;mr.mash estimated coefficients&quot;,
     col=colorz, cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
legend(&quot;topleft&quot;, 
       legend = c(&quot;Non-zero&quot;, &quot;Zero&quot;), 
       col = c(&quot;black&quot;, &quot;red&quot;), 
       pch = c(1, 1), 
       horiz = FALSE,
       cex=2) </code></pre>
<p><img src="figure/dense_scenario_issue.Rmd/plot%20results%20init%20V-1.png" width="1440" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-no-spaces-plot-results-init-V-1">
Past versions of “plot results init V-1.png”
</button>
</p>
<div id="fig-no-spaces-plot-results-init-V-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/fmorgante/mr_mash_test/blob/2ace633ad122eb0a8919a385a60a256d4d4312fc/docs/figure/dense_scenario_issue.Rmd/plot results init V-1.png" target="_blank">2ace633</a>
</td>
<td>
fmorgante
</td>
<td>
2020-06-02
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>print(&quot;Prediction performance with glmnet&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with glmnet&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest, Yhat_test_glmnet)</code></pre>
<pre><code>$bias
[1] 0.7973441 0.7171240 1.1201318 0.9992656 0.9375002

$r2
[1] 0.12560024 0.10044653 0.14357016 0.13204939 0.09516285

$mse
[1] 502.2840 531.2713 573.5326 575.3974 722.7162</code></pre>
<pre class="r"><code>print(&quot;Prediction performance with mr.mash&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with mr.mash&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest, Yhat_test_mrmash_initV)</code></pre>
<pre><code>$bias
[1] 2.249720 2.175435 3.415054 2.814300 3.008327

$r2
[1] 0.1300084 0.1116978 0.2339525 0.1826696 0.1530939

$mse
[1] 518.3294 530.2425 591.2687 587.2565 730.4772</code></pre>
<pre class="r"><code>print(&quot;True V (full data)&quot;)</code></pre>
<pre><code>[1] &quot;True V (full data)&quot;</code></pre>
<pre class="r"><code>out$V</code></pre>
<pre><code>         [,1]     [,2]     [,3]     [,4]     [,5]
[1,] 331.4137   0.0000   0.0000   0.0000   0.0000
[2,]   0.0000 331.4137   0.0000   0.0000   0.0000
[3,]   0.0000   0.0000 331.4137   0.0000   0.0000
[4,]   0.0000   0.0000   0.0000 331.4137   0.0000
[5,]   0.0000   0.0000   0.0000   0.0000 331.4137</code></pre>
<pre class="r"><code>print(&quot;Estimated V (training data)&quot;)</code></pre>
<pre><code>[1] &quot;Estimated V (training data)&quot;</code></pre>
<pre class="r"><code>fit_mrmash_initV$V</code></pre>
<pre><code>         Y1       Y2       Y3       Y4       Y5
Y1 600.7643 271.7859 213.0299 197.5317 231.7817
Y2 271.7859 581.4730 244.8228 243.7119 255.7840
Y3 213.0299 244.8228 542.5543 252.0104 239.4596
Y4 197.5317 243.7119 252.0104 532.4978 214.1496
Y5 231.7817 255.7840 239.4596 214.1496 516.2267</code></pre>
<p>Here, we can see that that initilizing the residual covariance to the truth leads to essentially the same solutions as using the default initialization method.</p>
<p>After modifying <em>mr.mash</em>, we can now update the residual covariance imposing a diagonal structure. Let’s see how that works.</p>
<pre class="r"><code>###Fit mr.mash
fit_mrmash_diagV &lt;- mr.mash(Xtrain, Ytrain, S0, tol=1e-2, convergence_criterion=&quot;ELBO&quot;, update_w0=TRUE, 
                            update_w0_method=&quot;EM&quot;, compute_ELBO=TRUE, standardize=TRUE, verbose=FALSE, update_V=TRUE,
                            update_V_method=&quot;diagonal&quot;, mu1_init=Bhat_glmnet, w0_threshold=1e-8)</code></pre>
<pre><code>Processing the inputs... Done!
Fitting the optimization algorithm... Done!
Processing the outputs... Done!
mr.mash successfully executed in 3.024115 minutes!</code></pre>
<pre class="r"><code>Yhat_test_mrmash_diagV &lt;- predict(fit_mrmash_diagV, Xtest)</code></pre>
<pre class="r"><code>layout(matrix(c(1, 1, 2, 2,
                1, 1, 2, 2,
                0, 3, 3, 0,
                0, 3, 3, 0), 4, 4, byrow = TRUE))

###Plot estimated vs true coeffcients
##mr.mash
plot(out$B, fit_mrmash_diagV$mu1, main=&quot;mr.mash&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
##glmnet
plot(out$B, Bhat_glmnet, main=&quot;glmnet&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)

###Plot mr.mash vs glmnet estimated coeffcients
colorz &lt;- matrix(&quot;black&quot;, nrow=p, ncol=r)
zeros &lt;- apply(out$B, 2, function(x) x==0)
for(i in 1:ncol(colorz)){
  colorz[zeros[, i], i] &lt;- &quot;red&quot;
}

plot(Bhat_glmnet, fit_mrmash_diagV$mu1, main=&quot;mr.mash vs glmnet&quot;, 
     xlab=&quot;glmnet estimated coefficients&quot;, ylab=&quot;mr.mash estimated coefficients&quot;,
     col=colorz, cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
legend(&quot;topleft&quot;, 
       legend = c(&quot;Non-zero&quot;, &quot;Zero&quot;), 
       col = c(&quot;black&quot;, &quot;red&quot;), 
       pch = c(1, 1), 
       horiz = FALSE,
       cex=2) </code></pre>
<p><img src="figure/dense_scenario_issue.Rmd/plot%20results%20diag%20V-1.png" width="1440" style="display: block; margin: auto;" /></p>
<pre class="r"><code>print(&quot;Prediction performance with glmnet&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with glmnet&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest, Yhat_test_glmnet)</code></pre>
<pre><code>$bias
[1] 0.7973441 0.7171240 1.1201318 0.9992656 0.9375002

$r2
[1] 0.12560024 0.10044653 0.14357016 0.13204939 0.09516285

$mse
[1] 502.2840 531.2713 573.5326 575.3974 722.7162</code></pre>
<pre class="r"><code>print(&quot;Prediction performance with mr.mash&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with mr.mash&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest, Yhat_test_mrmash_diagV)</code></pre>
<pre><code>$bias
[1] 0.4126154 0.3465257 0.5752667 0.5588387 0.5719653

$r2
[1] 0.08870657 0.06277994 0.14750399 0.14278998 0.12151611

$mse
[1] 628.5332 698.4433 623.4901 649.4620 762.7409</code></pre>
<pre class="r"><code>print(&quot;True V (full data)&quot;)</code></pre>
<pre><code>[1] &quot;True V (full data)&quot;</code></pre>
<pre class="r"><code>out$V</code></pre>
<pre><code>         [,1]     [,2]     [,3]     [,4]     [,5]
[1,] 331.4137   0.0000   0.0000   0.0000   0.0000
[2,]   0.0000 331.4137   0.0000   0.0000   0.0000
[3,]   0.0000   0.0000 331.4137   0.0000   0.0000
[4,]   0.0000   0.0000   0.0000 331.4137   0.0000
[5,]   0.0000   0.0000   0.0000   0.0000 331.4137</code></pre>
<pre class="r"><code>print(&quot;Estimated V (training data)&quot;)</code></pre>
<pre><code>[1] &quot;Estimated V (training data)&quot;</code></pre>
<pre class="r"><code>fit_mrmash_diagV$V</code></pre>
<pre><code>         Y1       Y2       Y3       Y4       Y5
Y1 397.6517   0.0000   0.0000   0.0000   0.0000
Y2   0.0000 340.6878   0.0000   0.0000   0.0000
Y3   0.0000   0.0000 335.3694   0.0000   0.0000
Y4   0.0000   0.0000   0.0000 337.8452   0.0000
Y5   0.0000   0.0000   0.0000   0.0000 319.6164</code></pre>
<p>Although the estimated residual covariance is much closer to the truth and now many coefficients are not overshrunk, prediction performance is still poor. Another possibility is that overshrinking of coefficients happens because the mixture weights are poorly initialized. Let’s now try to initialize the mixture weights to the truth.</p>
<pre class="r"><code>###Fit mr.mash
w0 &lt;- rep(0, length(S0))
w0[c(104, 111)] &lt;- c(0.5, 0.5)
fit_mrmash_diagV_initw0 &lt;- mr.mash(Xtrain, Ytrain, S0, w0=w0, tol=1e-2, convergence_criterion=&quot;ELBO&quot;, update_w0=TRUE, 
                                   update_w0_method=&quot;EM&quot;, compute_ELBO=TRUE, standardize=TRUE, verbose=FALSE, update_V=TRUE,
                                   update_V_method=&quot;diagonal&quot;, mu1_init=Bhat_glmnet, w0_threshold=1e-8)</code></pre>
<pre><code>Processing the inputs... Done!
Fitting the optimization algorithm... Done!
Processing the outputs... Done!
mr.mash successfully executed in 0.3478752 minutes!</code></pre>
<pre class="r"><code>Yhat_test_mrmash_diagV_initw0 &lt;- predict(fit_mrmash_diagV_initw0, Xtest)</code></pre>
<pre class="r"><code>layout(matrix(c(1, 1, 2, 2,
                1, 1, 2, 2,
                0, 3, 3, 0,
                0, 3, 3, 0), 4, 4, byrow = TRUE))

###Plot estimated vs true coeffcients
##mr.mash
plot(out$B, fit_mrmash_diagV_initw0$mu1, main=&quot;mr.mash&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
##glmnet
plot(out$B, Bhat_glmnet, main=&quot;glmnet&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)

###Plot mr.mash vs glmnet estimated coeffcients
colorz &lt;- matrix(&quot;black&quot;, nrow=p, ncol=r)
zeros &lt;- apply(out$B, 2, function(x) x==0)
for(i in 1:ncol(colorz)){
  colorz[zeros[, i], i] &lt;- &quot;red&quot;
}

plot(Bhat_glmnet, fit_mrmash_diagV_initw0$mu1, main=&quot;mr.mash vs glmnet&quot;, 
     xlab=&quot;glmnet estimated coefficients&quot;, ylab=&quot;mr.mash estimated coefficients&quot;,
     col=colorz, cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
legend(&quot;topleft&quot;, 
       legend = c(&quot;Non-zero&quot;, &quot;Zero&quot;), 
       col = c(&quot;black&quot;, &quot;red&quot;), 
       pch = c(1, 1), 
       horiz = FALSE,
       cex=2) </code></pre>
<p><img src="figure/dense_scenario_issue.Rmd/plot%20results%20diag%20V%20init%20w0-1.png" width="1440" style="display: block; margin: auto;" /></p>
<pre class="r"><code>print(&quot;Prediction performance with glmnet&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with glmnet&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest, Yhat_test_glmnet)</code></pre>
<pre><code>$bias
[1] 0.7973441 0.7171240 1.1201318 0.9992656 0.9375002

$r2
[1] 0.12560024 0.10044653 0.14357016 0.13204939 0.09516285

$mse
[1] 502.2840 531.2713 573.5326 575.3974 722.7162</code></pre>
<pre class="r"><code>print(&quot;Prediction performance with mr.mash&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with mr.mash&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest, Yhat_test_mrmash_diagV_initw0)</code></pre>
<pre><code>$bias
[1] 0.6137635 0.6313922 0.7989912 0.8056523 0.6864326

$r2
[1] 0.1287113 0.1368245 0.1864412 0.1935577 0.1148231

$mse
[1] 525.1827 533.7511 549.6033 545.9291 725.6260</code></pre>
<pre class="r"><code>print(&quot;True V (full data)&quot;)</code></pre>
<pre><code>[1] &quot;True V (full data)&quot;</code></pre>
<pre class="r"><code>out$V</code></pre>
<pre><code>         [,1]     [,2]     [,3]     [,4]     [,5]
[1,] 331.4137   0.0000   0.0000   0.0000   0.0000
[2,]   0.0000 331.4137   0.0000   0.0000   0.0000
[3,]   0.0000   0.0000 331.4137   0.0000   0.0000
[4,]   0.0000   0.0000   0.0000 331.4137   0.0000
[5,]   0.0000   0.0000   0.0000   0.0000 331.4137</code></pre>
<pre class="r"><code>print(&quot;Estimated V (training data)&quot;)</code></pre>
<pre><code>[1] &quot;Estimated V (training data)&quot;</code></pre>
<pre class="r"><code>fit_mrmash_diagV_initw0$V</code></pre>
<pre><code>         Y1       Y2       Y3       Y4       Y5
Y1 407.9608   0.0000   0.0000   0.0000   0.0000
Y2   0.0000 346.9626   0.0000   0.0000   0.0000
Y3   0.0000   0.0000 349.2363   0.0000   0.0000
Y4   0.0000   0.0000   0.0000 345.4373   0.0000
Y5   0.0000   0.0000   0.0000   0.0000 328.7146</code></pre>
<p>Now, prediction performance is better. Next, let’s try to fix the mixture weights to the truth.</p>
<pre class="r"><code>###Fit mr.mash
fit_mrmash_diagV_truew0 &lt;- mr.mash(Xtrain, Ytrain, S0, w0=w0, tol=1e-2, convergence_criterion=&quot;ELBO&quot;, update_w0=FALSE, 
                                    update_w0_method=&quot;EM&quot;, compute_ELBO=TRUE, standardize=TRUE, verbose=FALSE, update_V=TRUE,
                                    update_V_method=&quot;diagonal&quot;, mu1_init=Bhat_glmnet, w0_threshold=1e-8)</code></pre>
<pre><code>Processing the inputs... Done!
Fitting the optimization algorithm... Done!
Processing the outputs... Done!
mr.mash successfully executed in 0.7530105 minutes!</code></pre>
<pre class="r"><code>Yhat_test_mrmash_diagV_truew0 &lt;- predict(fit_mrmash_diagV_truew0, Xtest)</code></pre>
<pre class="r"><code>layout(matrix(c(1, 1, 2, 2,
                1, 1, 2, 2,
                0, 3, 3, 0,
                0, 3, 3, 0), 4, 4, byrow = TRUE))

###Plot estimated vs true coeffcients
##mr.mash
plot(out$B, fit_mrmash_diagV_truew0$mu1, main=&quot;mr.mash&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
##glmnet
plot(out$B, Bhat_glmnet, main=&quot;glmnet&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)

###Plot mr.mash vs glmnet estimated coeffcients
colorz &lt;- matrix(&quot;black&quot;, nrow=p, ncol=r)
zeros &lt;- apply(out$B, 2, function(x) x==0)
for(i in 1:ncol(colorz)){
  colorz[zeros[, i], i] &lt;- &quot;red&quot;
}

plot(Bhat_glmnet, fit_mrmash_diagV_truew0$mu1, main=&quot;mr.mash vs glmnet&quot;, 
     xlab=&quot;glmnet estimated coefficients&quot;, ylab=&quot;mr.mash estimated coefficients&quot;,
     col=colorz, cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
legend(&quot;topleft&quot;, 
       legend = c(&quot;Non-zero&quot;, &quot;Zero&quot;), 
       col = c(&quot;black&quot;, &quot;red&quot;), 
       pch = c(1, 1), 
       horiz = FALSE,
       cex=2) </code></pre>
<p><img src="figure/dense_scenario_issue.Rmd/plot%20results%20diag%20V%20true%20w0-1.png" width="1440" style="display: block; margin: auto;" /></p>
<pre class="r"><code>print(&quot;Prediction performance with glmnet&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with glmnet&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest, Yhat_test_glmnet)</code></pre>
<pre><code>$bias
[1] 0.7973441 0.7171240 1.1201318 0.9992656 0.9375002

$r2
[1] 0.12560024 0.10044653 0.14357016 0.13204939 0.09516285

$mse
[1] 502.2840 531.2713 573.5326 575.3974 722.7162</code></pre>
<pre class="r"><code>print(&quot;Prediction performance with mr.mash&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with mr.mash&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest, Yhat_test_mrmash_diagV_truew0)</code></pre>
<pre><code>$bias
[1] 0.7032514 0.7018678 0.9482747 0.9433495 0.8535756

$r2
[1] 0.1447663 0.1448465 0.2249874 0.2273485 0.1521071

$mse
[1] 501.5878 516.7773 516.6486 516.4104 680.2251</code></pre>
<pre class="r"><code>print(&quot;True V (full data)&quot;)</code></pre>
<pre><code>[1] &quot;True V (full data)&quot;</code></pre>
<pre class="r"><code>out$V</code></pre>
<pre><code>         [,1]     [,2]     [,3]     [,4]     [,5]
[1,] 331.4137   0.0000   0.0000   0.0000   0.0000
[2,]   0.0000 331.4137   0.0000   0.0000   0.0000
[3,]   0.0000   0.0000 331.4137   0.0000   0.0000
[4,]   0.0000   0.0000   0.0000 331.4137   0.0000
[5,]   0.0000   0.0000   0.0000   0.0000 331.4137</code></pre>
<pre class="r"><code>print(&quot;Estimated V (training data)&quot;)</code></pre>
<pre><code>[1] &quot;Estimated V (training data)&quot;</code></pre>
<pre class="r"><code>fit_mrmash_diagV_truew0$V</code></pre>
<pre><code>         Y1       Y2       Y3      Y4       Y5
Y1 419.9455   0.0000   0.0000   0.000   0.0000
Y2   0.0000 349.1219   0.0000   0.000   0.0000
Y3   0.0000   0.0000 355.0746   0.000   0.0000
Y4   0.0000   0.0000   0.0000 355.097   0.0000
Y5   0.0000   0.0000   0.0000   0.000 333.5331</code></pre>
<p>Prediction accuracy is now good and better than group-lasso. Finally, let’s fix both the residual covariance and mixture weights to the truth.</p>
<pre class="r"><code>###Fit mr.mash
fit_mrmash_trueV_truew0 &lt;- mr.mash(Xtrain, Ytrain, S0, w0=w0, tol=1e-2, convergence_criterion=&quot;ELBO&quot;, update_w0=FALSE, 
                                    update_w0_method=&quot;EM&quot;, compute_ELBO=TRUE, standardize=TRUE, verbose=FALSE, update_V=FALSE,
                                    update_V_method=&quot;diagonal&quot;, mu1_init=Bhat_glmnet, w0_threshold=1e-8, V=out$V)</code></pre>
<pre><code>Processing the inputs... Done!
Fitting the optimization algorithm... Done!
Processing the outputs... Done!
mr.mash successfully executed in 0.7278353 minutes!</code></pre>
<pre class="r"><code>Yhat_test_mrmash_trueV_truew0 &lt;- predict(fit_mrmash_trueV_truew0, Xtest)</code></pre>
<pre class="r"><code>layout(matrix(c(1, 1, 2, 2,
                1, 1, 2, 2,
                0, 3, 3, 0,
                0, 3, 3, 0), 4, 4, byrow = TRUE))

###Plot estimated vs true coeffcients
##mr.mash
plot(out$B, fit_mrmash_trueV_truew0$mu1, main=&quot;mr.mash&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
##glmnet
plot(out$B, Bhat_glmnet, main=&quot;glmnet&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)

###Plot mr.mash vs glmnet estimated coeffcients
colorz &lt;- matrix(&quot;black&quot;, nrow=p, ncol=r)
zeros &lt;- apply(out$B, 2, function(x) x==0)
for(i in 1:ncol(colorz)){
  colorz[zeros[, i], i] &lt;- &quot;red&quot;
}

plot(Bhat_glmnet, fit_mrmash_trueV_truew0$mu1, main=&quot;mr.mash vs glmnet&quot;, 
     xlab=&quot;glmnet estimated coefficients&quot;, ylab=&quot;mr.mash estimated coefficients&quot;,
     col=colorz, cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
legend(&quot;topleft&quot;, 
       legend = c(&quot;Non-zero&quot;, &quot;Zero&quot;), 
       col = c(&quot;black&quot;, &quot;red&quot;), 
       pch = c(1, 1), 
       horiz = FALSE,
       cex=2) </code></pre>
<p><img src="figure/dense_scenario_issue.Rmd/plot%20results%20true%20V%20true%20w0-1.png" width="1440" style="display: block; margin: auto;" /></p>
<pre class="r"><code>print(&quot;Prediction performance with glmnet&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with glmnet&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest, Yhat_test_glmnet)</code></pre>
<pre><code>$bias
[1] 0.7973441 0.7171240 1.1201318 0.9992656 0.9375002

$r2
[1] 0.12560024 0.10044653 0.14357016 0.13204939 0.09516285

$mse
[1] 502.2840 531.2713 573.5326 575.3974 722.7162</code></pre>
<pre class="r"><code>print(&quot;Prediction performance with mr.mash&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with mr.mash&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest, Yhat_test_mrmash_trueV_truew0)</code></pre>
<pre><code>$bias
[1] 0.7029293 0.6891626 0.9544747 0.9387136 0.8622740

$r2
[1] 0.1516346 0.1464096 0.2389721 0.2360162 0.1627364

$mse
[1] 498.3975 517.7781 507.2839 510.7310 671.4638</code></pre>
<pre class="r"><code>print(&quot;True V (full data)&quot;)</code></pre>
<pre><code>[1] &quot;True V (full data)&quot;</code></pre>
<pre class="r"><code>out$V</code></pre>
<pre><code>         [,1]     [,2]     [,3]     [,4]     [,5]
[1,] 331.4137   0.0000   0.0000   0.0000   0.0000
[2,]   0.0000 331.4137   0.0000   0.0000   0.0000
[3,]   0.0000   0.0000 331.4137   0.0000   0.0000
[4,]   0.0000   0.0000   0.0000 331.4137   0.0000
[5,]   0.0000   0.0000   0.0000   0.0000 331.4137</code></pre>
<pre class="r"><code>print(&quot;Estimated V (training data)&quot;)</code></pre>
<pre><code>[1] &quot;Estimated V (training data)&quot;</code></pre>
<pre class="r"><code>fit_mrmash_trueV_truew0$V</code></pre>
<pre><code>         Y1       Y2       Y3       Y4       Y5
Y1 331.4137   0.0000   0.0000   0.0000   0.0000
Y2   0.0000 331.4137   0.0000   0.0000   0.0000
Y3   0.0000   0.0000 331.4137   0.0000   0.0000
Y4   0.0000   0.0000   0.0000 331.4137   0.0000
Y5   0.0000   0.0000   0.0000   0.0000 331.4137</code></pre>
<p>Not surprisingly we now achieve the best accuracy of all models.</p>
<p>However, looking at ELBO for all the models fitted so far reveals something interesting. The models that provide the best prediction performance in the test set are those with smaller ELBO, and viceversa. My intuition is that the largest ELBO is due to those models having more free parameters (since the include updating the weights and V with no structure imposed).</p>
<pre class="r"><code>print(paste(&quot;Update w0, update V:&quot;, fit_mrmash$ELBO))</code></pre>
<pre><code>[1] &quot;Update w0, update V: -10789.3132448488&quot;</code></pre>
<pre class="r"><code>print(paste(&quot;Update w0, fix V to the truth:&quot;, fit_mrmash_fixV$ELBO))</code></pre>
<pre><code>[1] &quot;Update w0, fix V to the truth: -10858.0496418998&quot;</code></pre>
<pre class="r"><code>print(paste(&quot;Update w0, update V initializing it to the truth:&quot;, fit_mrmash_initV$ELBO))</code></pre>
<pre><code>[1] &quot;Update w0, update V initializing it to the truth: -10789.303405305&quot;</code></pre>
<pre class="r"><code>print(paste(&quot;Update w0, update V imposing a diagonal structure:&quot;, fit_mrmash_diagV$ELBO))</code></pre>
<pre><code>[1] &quot;Update w0, update V imposing a diagonal structure: -10855.1638789792&quot;</code></pre>
<pre class="r"><code>print(paste(&quot;Update w0 initializing them to the truth, update V imposing a diagonal structure:&quot;, fit_mrmash_diagV_initw0$ELBO))</code></pre>
<pre><code>[1] &quot;Update w0 initializing them to the truth, update V imposing a diagonal structure: -10869.1875885718&quot;</code></pre>
<pre class="r"><code>print(paste(&quot;Fix w0 to the truth, update V imposing a diagonal structure:&quot;, fit_mrmash_diagV_truew0$ELBO))</code></pre>
<pre><code>[1] &quot;Fix w0 to the truth, update V imposing a diagonal structure: -10936.6653594846&quot;</code></pre>
<pre class="r"><code>print(paste(&quot;Fix w0 and V to the truth:&quot;, fit_mrmash_trueV_truew0$ELBO))</code></pre>
<pre><code>[1] &quot;Fix w0 and V to the truth: -10942.8402970471&quot;</code></pre>
</div>
<div id="sparser-scenario" class="section level3">
<h3>Sparser scenario</h3>
<p>For comparison let’s try to repeat the analysis above (updating V) on a sparser scenario, i.e., p_causal=50.</p>
<pre class="r"><code>###Set parameters
n &lt;- 600
p &lt;- 1000
p_causal &lt;- 50
r &lt;- 5
r_causal &lt;- r
pve &lt;- 0.5
B_cor &lt;- 1
X_cor &lt;- 0
V_cor &lt;- 0

###Simulate V, B, X and Y
out50 &lt;- mr.mash.alpha:::simulate_mr_mash_data(n, p, p_causal, r, r_causal=r, intercepts = rep(1, r),
                                             pve=pve, B_cor=B_cor, B_scale=0.8, X_cor=X_cor, X_scale=0.8, V_cor=V_cor)

colnames(out50$Y) &lt;- paste0(&quot;Y&quot;, seq(1, r))
rownames(out50$Y) &lt;- paste0(&quot;N&quot;, seq(1, n))
colnames(out50$X) &lt;- paste0(&quot;X&quot;, seq(1, p))
rownames(out50$X) &lt;- paste0(&quot;N&quot;, seq(1, n))

###Split the data in training and test sets
test_set50 &lt;- sort(sample(x=c(1:n), size=round(n*0.2), replace=FALSE))
Ytrain50 &lt;- out50$Y[-test_set50, ]
Xtrain50 &lt;- out50$X[-test_set50, ]
Ytest50 &lt;- out50$Y[test_set50, ]
Xtest50 &lt;- out50$X[test_set50, ]

###Fit group-lasso
cvfit_glmnet50 &lt;- cv.glmnet(x=Xtrain50, y=Ytrain50, family=&quot;mgaussian&quot;, alpha=1)
coeff_glmnet50 &lt;- coef(cvfit_glmnet50, s=&quot;lambda.min&quot;)
Bhat_glmnet50 &lt;- matrix(as.numeric(NA), nrow=p, ncol=r)
ahat_glmnet50 &lt;- vector(&quot;numeric&quot;, r)
for(i in 1:length(coeff_glmnet50)){
  Bhat_glmnet50[, i] &lt;- as.vector(coeff_glmnet50[[i]])[-1]
  ahat_glmnet50[i] &lt;- as.vector(coeff_glmnet50[[i]])[1]
}
Yhat_test_glmnet50 &lt;- drop(predict(cvfit_glmnet50, newx=Xtest50, s=&quot;lambda.min&quot;))

###Fit mr.mash
fit_mrmash50 &lt;- mr.mash(Xtrain50, Ytrain50, S0, tol=1e-2, convergence_criterion=&quot;ELBO&quot;, update_w0=TRUE, 
                     update_w0_method=&quot;EM&quot;, compute_ELBO=TRUE, standardize=TRUE, verbose=FALSE, update_V=TRUE,
                     mu1_init = Bhat_glmnet50, w0_threshold=1e-8)</code></pre>
<pre><code>Processing the inputs... Done!
Fitting the optimization algorithm... Done!
Processing the outputs... Done!
mr.mash successfully executed in 0.9003217 minutes!</code></pre>
<pre class="r"><code>Yhat_test_mrmash50 &lt;- predict(fit_mrmash50, Xtest50)</code></pre>
<pre class="r"><code>layout(matrix(c(1, 1, 2, 2,
                1, 1, 2, 2,
                0, 3, 3, 0,
                0, 3, 3, 0), 4, 4, byrow = TRUE))

###Plot estimated vs true coeffcients
##mr.mash
plot(out50$B, fit_mrmash50$mu1, main=&quot;mr.mash&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
##glmnet
plot(out50$B, Bhat_glmnet50, main=&quot;glmnet&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)

###Plot mr.mash vs glmnet estimated coeffcients
colorz &lt;- matrix(&quot;black&quot;, nrow=p, ncol=r)
zeros &lt;- apply(out50$B, 2, function(x) x==0)
for(i in 1:ncol(colorz)){
  colorz[zeros[, i], i] &lt;- &quot;red&quot;
}

plot(Bhat_glmnet50, fit_mrmash50$mu1, main=&quot;mr.mash vs glmnet&quot;, 
     xlab=&quot;glmnet estimated coefficients&quot;, ylab=&quot;mr.mash estimated coefficients&quot;,
     col=colorz, cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
legend(&quot;topleft&quot;, 
       legend = c(&quot;Non-zero&quot;, &quot;Zero&quot;), 
       col = c(&quot;black&quot;, &quot;red&quot;), 
       pch = c(1, 1), 
       horiz = FALSE,
       cex=2) </code></pre>
<p><img src="figure/dense_scenario_issue.Rmd/plot%20results%2050-1.png" width="1440" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-no-spaces-plot-results-50-1">
Past versions of “plot results 50-1.png”
</button>
</p>
<div id="fig-no-spaces-plot-results-50-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/fmorgante/mr_mash_test/blob/42ec51cccb6bdad3adfe121582e30dcf3a409008/docs/figure/dense_scenario_issue.Rmd/plot results 50-1.png" target="_blank">42ec51c</a>
</td>
<td>
fmorgante
</td>
<td>
2020-06-01
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>print(&quot;Prediction performance with glmnet&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with glmnet&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest50, Yhat_test_glmnet50)</code></pre>
<pre><code>$bias
[1] 1.1055781 1.2342390 0.8929952 1.2093933 1.1997154

$r2
[1] 0.3720343 0.4451268 0.2709590 0.3909355 0.4424055

$mse
[1] 50.11301 49.98023 54.17894 53.96508 46.36995</code></pre>
<pre class="r"><code>print(&quot;Prediction performance with mr.mash&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with mr.mash&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest50, Yhat_test_mrmash50)</code></pre>
<pre><code>$bias
[1] 0.9295387 1.1205934 0.8382439 0.9741666 1.0034935

$r2
[1] 0.4227190 0.5544520 0.3641726 0.4142314 0.4747928

$mse
[1] 46.04314 39.62819 48.00426 51.00798 42.98684</code></pre>
<pre class="r"><code>print(&quot;True V (full data)&quot;)</code></pre>
<pre><code>[1] &quot;True V (full data)&quot;</code></pre>
<pre class="r"><code>out50$V</code></pre>
<pre><code>       [,1]   [,2]   [,3]   [,4]   [,5]
[1,] 43.262  0.000  0.000  0.000  0.000
[2,]  0.000 43.262  0.000  0.000  0.000
[3,]  0.000  0.000 43.262  0.000  0.000
[4,]  0.000  0.000  0.000 43.262  0.000
[5,]  0.000  0.000  0.000  0.000 43.262</code></pre>
<pre class="r"><code>print(&quot;Estimated V (training data)&quot;)</code></pre>
<pre><code>[1] &quot;Estimated V (training data)&quot;</code></pre>
<pre class="r"><code>fit_mrmash50$V</code></pre>
<pre><code>           Y1        Y2         Y3        Y4         Y5
Y1 37.0783688  2.693322  2.7229456  3.426152 -0.6935028
Y2  2.6933223 42.243137  2.0293575  1.966294  2.6486737
Y3  2.7229456  2.029357 37.9331012 -2.456907 -0.2269764
Y4  3.4261520  1.966294 -2.4569072 42.752140 -3.0717702
Y5 -0.6935028  2.648674 -0.2269764 -3.071770 43.5576891</code></pre>
<p>The results of this simulation look very good in terms of prediction performance, estimation of the coefficients, and estimation of the residual covariance.</p>
</div>
<div id="dense-scenario-fewer-variables" class="section level3">
<h3>Dense scenario (fewer variables)</h3>
<p>Just as an additional test let’s now try to maintain the same signal-to-noise ratio as the first simulation but with fewer total variables, i.e., p=500 and p_causal=250.</p>
<pre class="r"><code>###Set parameters
n &lt;- 600
p &lt;- 500
p_causal &lt;- 250
r &lt;- 5
r_causal &lt;- r
pve &lt;- 0.5
B_cor &lt;- 1
X_cor &lt;- 0
V_cor &lt;- 0

###Simulate V, B, X and Y
out500 &lt;- mr.mash.alpha:::simulate_mr_mash_data(n, p, p_causal, r, r_causal=r, intercepts = rep(1, r),
                                             pve=pve, B_cor=B_cor, B_scale=0.8, X_cor=X_cor, X_scale=0.8, V_cor=V_cor)

colnames(out500$Y) &lt;- paste0(&quot;Y&quot;, seq(1, r))
rownames(out500$Y) &lt;- paste0(&quot;N&quot;, seq(1, n))
colnames(out500$X) &lt;- paste0(&quot;X&quot;, seq(1, p))
rownames(out500$X) &lt;- paste0(&quot;N&quot;, seq(1, n))

###Split the data in training and test sets
test_set500 &lt;- sort(sample(x=c(1:n), size=round(n*0.2), replace=FALSE))
Ytrain500 &lt;- out500$Y[-test_set500, ]
Xtrain500 &lt;- out500$X[-test_set500, ]
Ytest500 &lt;- out500$Y[test_set500, ]
Xtest500 &lt;- out500$X[test_set500, ]

###Fit group-lasso
cvfit_glmnet500 &lt;- cv.glmnet(x=Xtrain500, y=Ytrain500, family=&quot;mgaussian&quot;, alpha=1)
coeff_glmnet500 &lt;- coef(cvfit_glmnet500, s=&quot;lambda.min&quot;)
Bhat_glmnet500 &lt;- matrix(as.numeric(NA), nrow=p, ncol=r)
ahat_glmnet500 &lt;- vector(&quot;numeric&quot;, r)
for(i in 1:length(coeff_glmnet500)){
  Bhat_glmnet500[, i] &lt;- as.vector(coeff_glmnet500[[i]])[-1]
  ahat_glmnet500[i] &lt;- as.vector(coeff_glmnet500[[i]])[1]
}
Yhat_test_glmnet500 &lt;- drop(predict(cvfit_glmnet500, newx=Xtest500, s=&quot;lambda.min&quot;))

###Fit mr.mash
fit_mrmash500 &lt;- mr.mash(Xtrain500, Ytrain500, S0, tol=1e-2, convergence_criterion=&quot;ELBO&quot;, update_w0=TRUE, 
                     update_w0_method=&quot;EM&quot;, compute_ELBO=TRUE, standardize=TRUE, verbose=FALSE, update_V=TRUE,
                     mu1_init = Bhat_glmnet500, w0_threshold=1e-8)</code></pre>
<pre><code>Processing the inputs... Done!
Fitting the optimization algorithm... Done!
Processing the outputs... Done!
mr.mash successfully executed in 1.150798 minutes!</code></pre>
<pre class="r"><code>Yhat_test_mrmash500 &lt;- predict(fit_mrmash500, Xtest500)</code></pre>
<pre class="r"><code>layout(matrix(c(1, 1, 2, 2,
                1, 1, 2, 2,
                0, 3, 3, 0,
                0, 3, 3, 0), 4, 4, byrow = TRUE))

###Plot estimated vs true coeffcients
##mr.mash
plot(out500$B, fit_mrmash500$mu1, main=&quot;mr.mash&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
##glmnet
plot(out500$B, Bhat_glmnet500, main=&quot;glmnet&quot;, xlab=&quot;True coefficients&quot;, ylab=&quot;Estimated coefficients&quot;,
     cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)

###Plot mr.mash vs glmnet estimated coeffcients
colorz &lt;- matrix(&quot;black&quot;, nrow=p, ncol=r)
zeros &lt;- apply(out500$B, 2, function(x) x==0)
for(i in 1:ncol(colorz)){
  colorz[zeros[, i], i] &lt;- &quot;red&quot;
}

plot(Bhat_glmnet500, fit_mrmash500$mu1, main=&quot;mr.mash vs glmnet&quot;, 
     xlab=&quot;glmnet estimated coefficients&quot;, ylab=&quot;mr.mash estimated coefficients&quot;,
     col=colorz, cex=2, cex.lab=1.8, cex.main=2, cex.axis=1.8)
legend(&quot;topleft&quot;, 
       legend = c(&quot;Non-zero&quot;, &quot;Zero&quot;), 
       col = c(&quot;black&quot;, &quot;red&quot;), 
       pch = c(1, 1), 
       horiz = FALSE,
       cex=2) </code></pre>
<p><img src="figure/dense_scenario_issue.Rmd/plot%20results%20500-1.png" width="1440" style="display: block; margin: auto;" /></p>
<p>
<button type="button" class="btn btn-default btn-xs btn-workflowr btn-workflowr-fig" data-toggle="collapse" data-target="#fig-no-spaces-plot-results-500-1">
Past versions of “plot results 500-1.png”
</button>
</p>
<div id="fig-no-spaces-plot-results-500-1" class="collapse">
<div class="table-responsive">
<table class="table table-condensed table-hover">
<thead>
<tr>
<th>
Version
</th>
<th>
Author
</th>
<th>
Date
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<a href="https://github.com/fmorgante/mr_mash_test/blob/42ec51cccb6bdad3adfe121582e30dcf3a409008/docs/figure/dense_scenario_issue.Rmd/plot results 500-1.png" target="_blank">42ec51c</a>
</td>
<td>
fmorgante
</td>
<td>
2020-06-01
</td>
</tr>
</tbody>
</table>
</div>
</div>
<pre class="r"><code>print(&quot;Prediction performance with glmnet&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with glmnet&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest500, Yhat_test_glmnet500)</code></pre>
<pre><code>$bias
[1] 0.7460558 0.8379627 0.8827708 1.0150154 0.8988217

$r2
[1] 0.1566151 0.1754140 0.2246133 0.2126951 0.2354353

$mse
[1] 262.5938 307.5913 280.3553 259.2769 265.2623</code></pre>
<pre class="r"><code>print(&quot;Prediction performance with mr.mash&quot;)</code></pre>
<pre><code>[1] &quot;Prediction performance with mr.mash&quot;</code></pre>
<pre class="r"><code>compute_accuracy(Ytest500, Yhat_test_mrmash500)</code></pre>
<pre><code>$bias
[1] 0.8007963 1.0543346 0.9437460 0.9812104 1.0269983

$r2
[1] 0.1917870 0.2740736 0.2402338 0.2626540 0.2816563

$mse
[1] 249.0113 269.0810 268.4814 242.8198 247.6815</code></pre>
<pre class="r"><code>print(&quot;True V (full data)&quot;)</code></pre>
<pre><code>[1] &quot;True V (full data)&quot;</code></pre>
<pre class="r"><code>out500$V</code></pre>
<pre><code>         [,1]     [,2]     [,3]     [,4]     [,5]
[1,] 168.3364   0.0000   0.0000   0.0000   0.0000
[2,]   0.0000 168.3364   0.0000   0.0000   0.0000
[3,]   0.0000   0.0000 168.3364   0.0000   0.0000
[4,]   0.0000   0.0000   0.0000 168.3364   0.0000
[5,]   0.0000   0.0000   0.0000   0.0000 168.3364</code></pre>
<pre class="r"><code>print(&quot;Estimated V (training data)&quot;)</code></pre>
<pre><code>[1] &quot;Estimated V (training data)&quot;</code></pre>
<pre class="r"><code>fit_mrmash500$V</code></pre>
<pre><code>           Y1         Y2        Y3        Y4         Y5
Y1 190.385218   4.826421  31.00999  22.18042   5.743413
Y2   4.826421 181.999035  16.65374   8.40475   9.590063
Y3  31.009989  16.653736 198.77957  39.23005  35.494926
Y4  22.180421   8.404750  39.23005 209.04480  25.167324
Y5   5.743413   9.590063  35.49493  25.16732 194.837000</code></pre>
<p>In this last simulation, we can see that <em>mr.mash</em> performs pretty well, even in a denser scenario with fewer variables.</p>
<br>
<p>
<button type="button" class="btn btn-default btn-workflowr btn-workflowr-sessioninfo" data-toggle="collapse" data-target="#workflowr-sessioninfo" style="display: block;">
<span class="glyphicon glyphicon-wrench" aria-hidden="true"></span> Session information
</button>
</p>
<div id="workflowr-sessioninfo" class="collapse">
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 3.5.1 (2018-07-02)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Scientific Linux 7.4 (Nitrogen)

Matrix products: default
BLAS/LAPACK: /software/openblas-0.2.19-el7-x86_64/lib/libopenblas_haswellp-r0.2.19.so

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] glmnet_2.0-16        foreach_1.4.4        Matrix_1.2-15       
[4] mr.mash.alpha_0.1-76

loaded via a namespace (and not attached):
 [1] MBSP_1.0           Rcpp_1.0.4.6       compiler_3.5.1    
 [4] later_0.7.5        git2r_0.26.1       workflowr_1.6.1   
 [7] iterators_1.0.10   tools_3.5.1        digest_0.6.25     
[10] evaluate_0.12      lattice_0.20-38    GIGrvg_0.5        
[13] yaml_2.2.1         SparseM_1.77       mvtnorm_1.0-12    
[16] coda_0.19-3        stringr_1.4.0      knitr_1.20        
[19] fs_1.3.1           MatrixModels_0.4-1 rprojroot_1.3-2   
[22] grid_3.5.1         glue_1.4.0         R6_2.4.1          
[25] rmarkdown_1.10     mixsqp_0.3-43      irlba_2.3.3       
[28] magrittr_1.5       whisker_0.3-2      codetools_0.2-15  
[31] backports_1.1.5    promises_1.0.1     htmltools_0.3.6   
[34] matrixStats_0.55.0 mcmc_0.9-6         MASS_7.3-51.1     
[37] httpuv_1.4.5       quantreg_5.36      stringi_1.4.3     
[40] MCMCpack_1.4-4    </code></pre>
</div>
</div>


<!-- Adjust MathJax settings so that all math formulae are shown using
TeX fonts only; see
http://docs.mathjax.org/en/latest/configuration.html.  This will make
the presentation more consistent at the cost of the webpage sometimes
taking slightly longer to load. Note that this only works because the
footer is added to webpages before the MathJax javascript. -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
